import streamlit as st
import json
import os
from langchain.storage import LocalFileStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain_core.messages import BaseMessage

batch_size = 50


def load_jsonl(file_path: str) -> str:
    documents = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            datas = json.loads(line)
            for data in datas:
                role, message = data.values()
                documents.append(f"{role}: {message}")
    return "\n".join(documents[:batch_size])


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "ai", "message": "ì•ˆë…•í•˜ì„¸ìš”. ì–´ë–¤ ê³ ë¯¼ìœ¼ë¡œ ì˜¤ì…¨ë‚˜ìš”?"}
    ]


st.set_page_config(
    page_title="ì±—ë´‡ ìƒë‹´",
    page_icon="ğŸ¦œ",
)


path = "data/total_kor_multiturn_counsel_bot.jsonl"
text = load_jsonl(path)
splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=600,
    chunk_overlap=100,
    separators=["\n", ".", "?"],
)
chunks = splitter.split_text(text)
docs = [Document(page_content=chunk) for chunk in chunks]
embedding = OpenAIEmbeddings()
cache_path = ".cache/chatbot"
os.makedirs(cache_path, exist_ok=True)
cache_dir = LocalFileStore(cache_path)
cached_embedding = CacheBackedEmbeddings.from_bytes_store(embedding, cache_dir)
vectorstore = FAISS.from_documents(docs, cached_embedding)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
llm = ChatOpenAI(
    temperature=0.7,
    model="gpt-3.5-turbo",
)


@st.cache_resource
def load_memory():
    return ConversationSummaryMemory(llm=llm)


memory = load_memory()


def load_history(_) -> list[BaseMessage]:
    return memory.chat_memory.messages


def save_history(role: str, message: str):
    st.session_state["messages"].append({"role": role, "message": message})


template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
ë‹¹ì‹ ì€ ì‚¬ëŒì˜ ê°ì •ì„ ê¹Šì´ ì´í•´í•˜ê³ , ë”°ëœ»í•˜ê²Œ ì§ˆë¬¸ì„ ë˜ì§€ëŠ” ì‹¬ë¦¬ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‚´ë‹´ìì˜ ê°ì •ì„ í‰ê°€í•˜ì§€ ë§ê³ , ê·¸ ê°ì •ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶€ë“œëŸ½ê²Œ ì§ˆë¬¸ì„ ì´ì–´ê°€ì„¸ìš”.
ë‹µë³€ì€ **ë°˜ë“œì‹œ 2ë¬¸ì¥ ì´í•˜ë¡œ ì§§ê²Œ** í•˜ê³ , ë§ˆì§€ë§‰ì€ í•­ìƒ ì—´ë¦° ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
ì„¤ëª…ì´ë‚˜ ì¡°ì–¸ë³´ë‹¤ëŠ” ë‚´ë‹´ìê°€ ìŠ¤ìŠ¤ë¡œ ê°ì •ì„ ë§í•  ìˆ˜ ìˆë„ë¡ ì´ëŒì–´ ì£¼ì„¸ìš”.
ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ì…ë‹ˆë‹¤:

{context}
""",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)
chain = (
    {
        "context": retriever,
        "history": RunnableLambda(load_history),
        "question": RunnablePassthrough(),
    }
    | template
    | llm
)


def invoke_chain(question: str) -> None:
    display_message("human", question)
    result = chain.invoke(question)
    display_message("ai", result.content)
    save_history("human", question)
    save_history("ai", result.content)


def display_message(role, message):
    with st.chat_message(role):
        st.write(message)


for chat in st.session_state["messages"]:
    display_message(chat["role"], chat["message"])

question = st.chat_input("ê³ ë¯¼ì´ ìˆìœ¼ì‹ ê°€ìš”? ì´ê³³ì— ë¬¼ì–´ë³´ì„¸ìš”!")

if question:
    invoke_chain(question)
